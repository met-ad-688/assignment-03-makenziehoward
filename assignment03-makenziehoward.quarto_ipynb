{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Assignment 03\n",
        "author:\n",
        "  - name: Makenzie Howard\n",
        "    affiliations:\n",
        "      - id: bu\n",
        "        name: Boston University\n",
        "        city: Boston\n",
        "        state: MA\n",
        "number-sections: true\n",
        "date: '2025-09-22'\n",
        "format:\n",
        "  html:\n",
        "    theme: cerulean\n",
        "    toc: true\n",
        "    toc-depth: 2\n",
        "    code-overflow: wrap\n",
        "  docx: \n",
        "    code-overflow: wrap\n",
        "  pdf:\n",
        "    code-overflow: wrap\n",
        "date-modified: today\n",
        "date-format: long\n",
        "--- \n",
        "\n",
        "# Load the Dataset"
      ],
      "id": "b42fae9e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "#| fig-align: center\n",
        "\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "from pyspark.sql import SparkSession\n",
        "import re\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from pyspark.sql.functions import col, split, explode, regexp_replace, transform, when\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col, monotonically_increasing_id\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "pio.renderers.default = \"notebook\"\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"LightcastData\").getOrCreate()\n",
        "\n",
        "# Load Data\n",
        "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"lightcast_job_postings.csv\")\n",
        "df.createOrReplaceTempView(\"job_postings\")\n",
        "\n",
        "# Show Schema and Sample Data\n",
        "print(\"---This is Diagnostic check, No need to print it in the final doc---\")\n",
        "\n",
        "df.printSchema() # comment this line when rendering the submission\n",
        "df.show(5)"
      ],
      "id": "acc37821",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Histogram of Salary Distribution\n",
        "fig = px.histogram(df.toPandas(), x=\"SALARY\", nbins=50, title=\"SalaryDistribution\")\n",
        "fig.update_layout(bargap0.1)"
      ],
      "id": "d5b71018",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preparation"
      ],
      "id": "0dafc8b4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 1: Casting salary and experience columns\n",
        "df = (\n",
        "    df\n",
        "    .withColumn(\"SALARY\", col(\"SALARY\").cast(\"float\"))\n",
        "    .withColumn(\"SALARY_FROM\", col(\"SALARY_FROM\").cast(\"float\"))\n",
        "    .withColumn(\"SALARY_TO\", col(\"SALARY_TO\").cast(\"float\"))\n",
        "    .withColumn(\"MIN_YEARS_EXPERIENCE\", col(\"MIN_YEARS_EXPERIENCE\").cast(\"float\"))\n",
        "    .withColumn(\"MAX_YEARS_EXPERIENCE\", col(\"MAX_YEARS_EXPERIENCE\").cast(\"float\"))\n",
        ")\n",
        "\n",
        "# Step 2: Computing medians for salary columns\n",
        "def compute_median(sdf, col_name):\n",
        "    q= sdf.approxQuantile(col_name, [0.5], 0.01)\n",
        "    return q[0] if q else None\n",
        "\n",
        "median_from = compute_median(df, \"SALARY_FROM\")\n",
        "median_to = compute_median(df, \"SALARY_TO\")\n",
        "\n",
        "print(\"Medians:\", median_from, median_to)\n",
        "\n",
        "# Step 3: Imputing missing salaries, but not experience\n",
        "impute = {}\n",
        "if median_from is not None:\n",
        "    impute[\"SALARY_FROM\"] = median_from\n",
        "if median_to is not None:\n",
        "    impute[\"SALARY_TO\"] = median_to\n",
        "if impute:\n",
        "    df = df.fillna(impute)\n",
        "\n",
        "# Step 5: Computing average salary\n",
        "df = df.withColumn(\"Average_Salary\", (col(\"SALARY_FROM\") + col(\"SALARY_TO\")) / 2)\n",
        "\n",
        "# Step 6: Selecting Required Columns\n",
        "export_cols = [\n",
        "  \"EDUCATION_LEVELS_NAME\",\n",
        "  \"REMOTE_TYPE_NAME\",\n",
        "  \"MAX_YEARS_EXPERIENCE\",\n",
        "  \"Average_Salary\",\n",
        "  \"LOT_V6_SPECIALIZED_OCCUPATION_NAME\",\n",
        "]\n",
        "df_selected = df.select(*[c for c in export_cols if c in df.columns])\n",
        "\n",
        "# Step 7: Saving to CSV\n",
        "pdf = df_selected.toPandas()   # OK for small/medium data\n",
        "pdf.to_csv(\"./data/lightcast_cleaned.csv\", index=False)\n",
        "\n",
        "print(\"Data Cleaning Complete. Rows retained:\", len(pdf))"
      ],
      "id": "eff62b2d",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "ec2-m03",
      "language": "python",
      "display_name": "Python (ec2-m03)",
      "path": "/home/ubuntu/.local/share/jupyter/kernels/ec2-m03"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}